# Microprofile server properties
server.port=8080

#smallrye.llm.plugin.chat-model.class=dev.langchain4j.model.azure.AzureOpenAiChatModel
#smallrye.llm.plugin.chat-model.config.api-key=${azure.openai.api.key}
#smallrye.llm.plugin.chat-model.config.endpoint=${azure.openai.endpoint}
#smallrye.llm.plugin.chat-model.config.service-version=2024-02-15-preview
#smallrye.llm.plugin.chat-model.config.deployment-name=${azure.openai.deployment.name}
#smallrye.llm.plugin.chat-model.config.temperature=0.1
#smallrye.llm.plugin.chat-model.config.topP=0.1
#smallrye.llm.plugin.chat-model.config.timeout=PT120S
#smallrye.llm.plugin.chat-model.config.max-retries=2
#smallrye.llm.plugin.chat-model.config.logRequestsAndResponses=true

#podman run -d --name ollama --replace --pull=always --restart=always -p 11434:11434 -v ollama:/root/.ollama --stop-signal=SIGKILL docker.io/ollama/ollama^C
#podman  exec -it ollama ollama run llama3.1


smallrye.llm.plugin.chat-model.class=dev.langchain4j.model.openai.OpenAiChatModel
smallrye.llm.plugin.chat-model.config.base-url=http://localhost:11434/v1
smallrye.llm.plugin.chat-model.config.api-key=ollama
smallrye.llm.plugin.chat-model.config.temperature=0.1
smallrye.llm.plugin.chat-model.config.max-retries=2
smallrye.llm.plugin.chat-model.config.model-name=llama3.1


smallrye.llm.plugin.docRagRetriever.class=dev.langchain4j.rag.content.retriever.EmbeddingStoreContentRetriever
smallrye.llm.plugin.docRagRetriever.config.embeddingStore=lookup:default
smallrye.llm.plugin.docRagRetriever.config.embeddingModel=lookup:default
smallrye.llm.plugin.docRagRetriever.config.maxResults=3
smallrye.llm.plugin.docRagRetriever.config.minScore=0.6



smallrye.llm.embedding.store.in-memory.file=embedding.json

# Chat memory configuration, used by ChatAiFactory
chat.memory.max.messages=20

# Fraud detection configuration, used by FraudAiFactory
fraud.memory.max.messages=20

# Location of documents to RAG
app.docs-for-rag.dir=docs-for-rag



